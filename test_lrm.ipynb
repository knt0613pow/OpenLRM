{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import math\n",
    "import os\n",
    "import imageio\n",
    "import mcubes\n",
    "import trimesh\n",
    "import numpy as np\n",
    "import argparse\n",
    "from PIL import Image\n",
    "\n",
    "from lrm.models.generator import LRMGenerator\n",
    "from lrm.cam_utils import build_camera_principle, build_camera_standard, center_looking_at_camera_pose\n",
    "from lrm.inferrer import LRMInferrer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer = LRMInferrer('openlrm-base-obj-1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(infer.model) # lrm.models.generator.LRMGenerator\n",
    "infer.model.encoder_feat_dim # 768\n",
    "infer.model.camera_embed_dim # 1024\n",
    "infer.model.encoder # DInoWrapper\n",
    "infer.model.camera_embedder \n",
    "# CameraEmbedder(\n",
    "#   (mlp): Sequential(\n",
    "#     (0): Linear(in_features=16, out_features=1024, bias=True)\n",
    "#     (1): SiLU()\n",
    "#     (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
    "#   )\n",
    "# )\n",
    "infer.model.transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer.infer_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image = './assets/sample_input/hydrant.png'\n",
    "source_image_size = infer.infer_kwargs['source_size']\n",
    "image = torch.tensor(np.array(Image.open(source_image))).permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "print(\"image shape :  \",image.shape)\n",
    "image = torch.nn.functional.interpolate(image, size=(source_image_size, source_image_size), mode='bicubic', align_corners=True)\n",
    "print(\"reshaped image shape :  \",image.shape)\n",
    "image = torch.clamp(image, 0, 1)\n",
    "image = image.to(infer.device)\n",
    "render_size = infer.infer_kwargs['render_size']\n",
    "mesh_size = 384\n",
    "mesh_thres = 3.0\n",
    "chunck_size = 2\n",
    "batch_size = 1\n",
    "\n",
    "source_camera = infer._default_source_camera(batch_size).to(infer.device)\n",
    "print(\"source_camera shape :  \",source_camera.shape)\n",
    "print(\"source_camera :  \\n\",source_camera.reshape(-1, 4, 4))\n",
    "render_cameras = infer._default_render_cameras(batch_size).to(infer.device)\n",
    "print(\"render_cameras shape :  \",render_cameras.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    planes = infer.model.forward_planes(image, source_camera)\n",
    "    print(planes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    N = image.shape[0]\n",
    "    print(\"image shape :  \",image.shape)\n",
    "    image_feat = infer.model.encoder(image)\n",
    "    print(\"image_feat shape :  \",image_feat.shape)\n",
    "    camera_embed = infer.model.camera_embedder(source_camera)\n",
    "    print(\"source_camera shape :  \",source_camera.shape)\n",
    "    print(\"camera_embed shape :  \",camera_embed.shape)\n",
    "    planes = infer.model.transformer(image_feat, camera_embed)\n",
    "    print(\"planes shape :  \",planes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from lrm.models.transformer import ConditionModulationBlock\n",
    "class TriplaneTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer with condition and modulation that generates a triplane representation.\n",
    "    \n",
    "    Reference:\n",
    "    Timm: https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py#L486\n",
    "    \"\"\"\n",
    "    def __init__(self, inner_dim: int, image_feat_dim: int, camera_embed_dim: int,\n",
    "                 triplane_low_res: int, triplane_high_res: int, triplane_dim: int,\n",
    "                 num_layers: int, num_heads: int,\n",
    "                 eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "\n",
    "        # attributes\n",
    "        self.triplane_low_res = triplane_low_res\n",
    "        self.triplane_high_res = triplane_high_res\n",
    "        self.triplane_dim = triplane_dim\n",
    "\n",
    "        # modules\n",
    "        # initialize pos_embed with 1/sqrt(dim) * N(0, 1)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 3*triplane_low_res**2, inner_dim) * (1. / inner_dim) ** 0.5)\n",
    "        self.layers = nn.ModuleList([\n",
    "            ConditionModulationBlock(\n",
    "                inner_dim=inner_dim, cond_dim=image_feat_dim, mod_dim=camera_embed_dim, num_heads=num_heads, eps=eps)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(inner_dim, eps=eps)\n",
    "        self.deconv = nn.ConvTranspose2d(inner_dim, triplane_dim, kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, image_feats, camera_embeddings):\n",
    "        # image_feats: [N, L_cond, D_cond]\n",
    "        # camera_embeddings: [N, D_mod]\n",
    "\n",
    "        assert image_feats.shape[0] == camera_embeddings.shape[0], \\\n",
    "            f\"Mismatched batch size: {image_feats.shape[0]} vs {camera_embeddings.shape[0]}\"\n",
    "\n",
    "        N = image_feats.shape[0]\n",
    "        H = W = self.triplane_low_res\n",
    "        L = 3 * H * W\n",
    "\n",
    "        x = self.pos_embed.repeat(N, 1, 1)  # [N, L, D]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, image_feats, camera_embeddings)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # separate each plane and apply deconv\n",
    "        x = x.view(N, 3, H, W, -1)\n",
    "        x = torch.einsum('nihwd->indhw', x)  # [3, N, D, H, W]\n",
    "        x = x.contiguous().view(3*N, -1, H, W)  # [3*N, D, H, W]\n",
    "        x = self.deconv(x)  # [3*N, D', H', W']\n",
    "        x = x.view(3, N, *x.shape[-3:])  # [3, N, D', H', W']\n",
    "        x = torch.einsum('indhw->nidhw', x)  # [N, 3, D', H', W']\n",
    "        x = x.contiguous()\n",
    "\n",
    "        assert self.triplane_high_res == x.shape[-2], \\\n",
    "            f\"Output triplane resolution does not match with expected: {x.shape[-2]} vs {self.triplane_high_res}\"\n",
    "        assert self.triplane_dim == x.shape[-3], \\\n",
    "            f\"Output triplane dimension does not match with expected: {x.shape[-3]} vs {self.triplane_dim}\"\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./.cache/openlrm-base-obj-1.0/model.pth', map_location=infer.device)\n",
    "checkpoint['kwargs']\n",
    "print(checkpoint['kwargs'])\n",
    "triplaneTrans = TriplaneTransformer(\n",
    "    inner_dim = checkpoint['kwargs']['model']['transformer_dim'],\n",
    "    num_layers=checkpoint['kwargs']['model']['transformer_layers'],\n",
    "    num_heads=checkpoint['kwargs']['model']['transformer_heads'],\n",
    "    image_feat_dim=768,\n",
    "    camera_embed_dim=checkpoint['kwargs']['model']['camera_embed_dim'],\n",
    "    triplane_low_res=checkpoint['kwargs']['model']['triplane_low_res'],\n",
    "    triplane_high_res=checkpoint['kwargs']['model']['triplane_high_res'],\n",
    "    triplane_dim=checkpoint['kwargs']['model']['triplane_dim'],\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer.model.transformer.pos_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(infer._default_intrinsics())\n",
    "# tensor([[384., 384.], fx fy\n",
    "        # [256., 256.], cx cy\n",
    "        # [512., 512.]]) w h\n",
    "print(infer._default_source_camera(batch_size).reshape(-1, 4, 4))\n",
    "# tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000], RT\n",
    "#          [ 0.0000,  0.0000, -1.0000, -2.0000],\n",
    "#          [ 0.0000,  1.0000,  0.0000,  0.0000],\n",
    "#          [ 0.7500,  0.7500,  0.5000,  0.5000]]]) 384/512\n",
    "print(infer._default_source_camera(batch_size).shape)\n",
    "print(infer._default_render_cameras(batch_size).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(infer._get_surrounding_views().shape) # defualt - 160 surrounding views RT\n",
    "print(infer._get_surrounding_views()[40])\n",
    "\n",
    "# print(infer._default_render_cameras(batch_size).shape)\n",
    "# print(infer._default_render_cameras(batch_size).reshape(-1, 4, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test source camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Loaded model from checkpoint ========\n",
      "image shape :   torch.Size([1, 3, 512, 512])\n",
      "reshaped image shape :   torch.Size([1, 3, 256, 256])\n",
      "source_camera :  \n",
      " tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -1.0000, -2.0000],\n",
      "         [ 0.0000,  1.0000,  0.0000,  0.0000],\n",
      "         [ 0.7500,  0.7500,  0.5000,  0.5000]]], device='cuda:0')\n",
      "source_camera2 : \n",
      " tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  1.0000,  0.0000, -2.0000],\n",
      "         [ 0.0000,  0.0000,  1.0000,  0.0000],\n",
      "         [ 0.7500,  0.7500,  0.5000,  0.5000]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from lrm.inferrer import LRMInferrer\n",
    "source_image = './assets/sample_input/hydrant.png'\n",
    "infer = LRMInferrer('openlrm-base-obj-1.0')\n",
    "source_image_size = infer.infer_kwargs['source_size']\n",
    "image = torch.tensor(np.array(Image.open(source_image))).permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "print(\"image shape :  \",image.shape)\n",
    "image = torch.nn.functional.interpolate(image, size=(source_image_size, source_image_size), mode='bicubic', align_corners=True)\n",
    "print(\"reshaped image shape :  \",image.shape)\n",
    "image = torch.clamp(image, 0, 1)\n",
    "image = image.to(infer.device)\n",
    "render_size = infer.infer_kwargs['render_size']\n",
    "mesh_size = 384\n",
    "mesh_thres = 3.0\n",
    "chunck_size = 2\n",
    "batch_size = 1\n",
    "\n",
    "source_camera = infer._default_source_camera(batch_size).to(infer.device)\n",
    "source_camera2 = infer._default_source_camera2(batch_size).to(infer.device)\n",
    "print(\"source_camera :  \\n\",source_camera.reshape(-1, 4, 4))\n",
    "print(\"source_camera2 : \\n\",source_camera2.reshape(-1, 4, 4))\n",
    "render_cameras = infer._default_render_cameras(batch_size).to(infer.device)\n",
    "# print(\"render_cameras shape :  \",render_cameras.shape)\n",
    "with torch.no_grad():\n",
    "    planes = infer.model.forward_planes(image, source_camera)\n",
    "    grid_out = infer.model.synthesizer.forward_grid(planes = planes, grid_size=mesh_size)\n",
    "    planes2 = infer.model.forward_planes(image, source_camera2)\n",
    "    grid_out2 = infer.model.synthesizer.forward_grid(planes = planes2, grid_size=mesh_size)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['rgb', 'sigma'])\n",
      "torch.Size([1, 384, 384, 384, 3])\n",
      "torch.Size([1, 384, 384, 384, 1])\n",
      "(268222, 3) (536468, 3) 2.0000894781188716\n",
      "[90.12259924 99.13544342  0.2275228 ]\n",
      "[291.81161441 278.64620625 381.41512328]\n",
      "[-0.53061146 -0.48366957 -0.99881499] [0.51985216 0.45128232 0.9865371 ]\n"
     ]
    }
   ],
   "source": [
    "print(grid_out.keys())\n",
    "print(grid_out['rgb'].shape)\n",
    "print(grid_out['sigma'].shape)\n",
    "vtx, faces = mcubes.marching_cubes(grid_out['sigma'].squeeze(0).squeeze(-1).cpu().numpy(), mesh_thres)\n",
    "print(vtx.shape, faces.shape, faces.shape[0] / vtx.shape[0])\n",
    "print(vtx.min(axis = 0))\n",
    "print(vtx.max(axis = 0))\n",
    "vtx_post = vtx / mesh_size * 2 - 1\n",
    "print(vtx_post.min(axis = 0), vtx_post.max(axis = 0))\n",
    "vtx_tensor = torch.tensor(vtx_post, dtype = torch.float32, device = infer.device).unsqueeze(0)\n",
    "vtx_color  = infer.model.synthesizer.forward_points(planes , vtx_tensor)\n",
    "minmax = vtx_post.max(axis = 0) - vtx_post.min(axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['rgb', 'sigma'])\n",
      "torch.Size([1, 384, 384, 384, 3])\n",
      "torch.Size([1, 384, 384, 384, 1])\n",
      "(244114, 3) (488240, 3) 2.000049157360905\n",
      "[ 94.09826899 100.20328071  12.0248415 ]\n",
      "[287.78613444 275.51346265 371.76421675]\n",
      "[-0.50990485 -0.47810791 -0.93737062] [0.49888612 0.43496595 0.93627196]\n",
      "[1.00879097 0.91307386 1.87364258]\n"
     ]
    }
   ],
   "source": [
    "print(grid_out2.keys())\n",
    "print(grid_out2['rgb'].shape)\n",
    "print(grid_out2['sigma'].shape)\n",
    "vtx2, faces2 = mcubes.marching_cubes(grid_out2['sigma'].squeeze(0).squeeze(-1).cpu().numpy(), mesh_thres)\n",
    "print(vtx2.shape, faces2.shape, faces2.shape[0] / vtx2.shape[0])\n",
    "print(vtx2.min(axis = 0))\n",
    "print(vtx2.max(axis = 0))\n",
    "vtx_post2 = vtx2 / mesh_size * 2 - 1\n",
    "print(vtx_post2.min(axis = 0), vtx_post2.max(axis = 0))\n",
    "vtx_tensor2 = torch.tensor(vtx_post2, dtype = torch.float32, device = infer.device).unsqueeze(0)\n",
    "vtx_color2  = infer.model.synthesizer.forward_points(planes2 , vtx_tensor2)\n",
    "minmax2 = vtx_post2.max(axis = 0) - vtx_post2.min(axis = 0)\n",
    "print( vtx_post2.max(axis=0)- vtx_post2.min(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7836339522051574"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(minmax2 [2]/minmax2[0], minmax2[2]/minmax2[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LRM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
